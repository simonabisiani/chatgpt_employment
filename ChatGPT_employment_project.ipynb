{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO0INvZe6JIU1ylT/JkJLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simonabisiani/chatgpt_employment/blob/main/ChatGPT_employment_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT and employment project\n",
        "*Authors: Simona Bisiani, Al-Hussein Abutaleb*\n",
        "\n",
        "The purpose of this notebook is to perform and evaluate a text analysis of tweets relating to ChatGPT and employment. The project sits within research conducted at the Surrey Institute for Artificial Intelligence by Dr Eddy Zhu and Dr Erin Ling. \n",
        "\n",
        "The notebook includes the following sections:\n",
        "- data collection (this is not to be re-run, it's only included for transparency purposes)\n",
        "- summary statistics and characteristics of the data\n",
        "- topic modeling\n",
        "- sentiment analysis"
      ],
      "metadata": {
        "id": "AGLHX63XmHFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The necessary data for processing code in this notebook can be found at this [GitHub repository](https://github.com/simonabisiani/chatgpt_employment). We will use the code below to load the data in the notebook."
      ],
      "metadata": {
        "id": "xtUaLKOV6037"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING THE DATA\n",
        "!git clone https://github.com/simonabisiani/chatgpt_employment.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53gvcuHf6yIB",
        "outputId": "31280e36-ac67-4b88-ce52-4f58f7d3d1ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chatgpt_employment'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 41 (delta 7), reused 24 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (41/41), 6.11 MiB | 2.98 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 141.96 MiB | 40.72 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rpy2==3.5.1   # needed to run R code in colab; restart runtime after running this chunk"
      ],
      "metadata": {
        "id": "k1a2o9YWz80d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext rpy2.ipython\n",
        "\n",
        "# this command allows us to run both R and Python code throughout the notebook. \n",
        "#Whenever we need to run R code, we simply need to include %%R at the beginning of the code chunk"
      ],
      "metadata": {
        "id": "R2lf3DFHnxrd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data collection\n",
        "Upon recommendations from Dr Ling, we queried Twitter for tweets relating to search **chatgpt** *AND* any of the following words:\n",
        "**\"employment\", \"employability\", \"employer\",\"employee\", \"unemployment\",      \"unemployed\", \"job\", \"work\", \"skill\", \"taking over\", \"replace human\", \"performance\", \"concern\", \"insecurity\", \"fear\", \"threat\", \"opportunities\", \"training\",\"creation\", \"displacement\", \"occupation\", \"earning\",\"future demand\",\n",
        "\"education\", \"labour\", \"labor\", \"task\", \"industry\", \"workforce\", \"shortage\",\"collaboration\", \"collaborate\", \"human ai team\", \"team\", \"economic\", \"economy\"** \n",
        "\n",
        "On 24th February we collected, using the Twitter API and the R package *academictwitteR*, any tweets containing our query **between 30th November 2022 and 24th February 2023**. The data collection was only possible through an Academic Twitter API developer account, due to: the number of tweets wanted, and the possibility to search the full archive. With a standard developer account (e.g. Elevated), this would have not been possible.\n",
        "\n",
        "The code we wrote to collect the data is this:\n",
        "\n",
        "```\n",
        "chatgpt_full_collection <- data.frame()\n",
        "and <- \"chatgpt\"\n",
        "or <- c(\"employment\",\n",
        "        \"employability\",\n",
        "        \"employer\",\n",
        "        \"employee\",         \n",
        "        \"unemployment\",\n",
        "        \"unemployed\",\n",
        "        \"job\", \n",
        "        \"work\",\n",
        "        \"skill\",\n",
        "        \"taking over\",\n",
        "        \"replace human\",\n",
        "        \"performance\",\n",
        "        \"concern\",\n",
        "        \"insecurity\",\n",
        "        \"fear\",\n",
        "        \"threat\",\n",
        "        \"opportunities\",\n",
        "        \"training\",\n",
        "        \"creation\", \n",
        "        \"displacement\", \n",
        "        \"occupation\",\n",
        "        \"earning\", \n",
        "        \"future demand\",\n",
        "        \"education\",\n",
        "        \"labour\",\n",
        "        \"labor\", \n",
        "        \"task\", \n",
        "        \"industry\",\n",
        "        \"workforce\",\n",
        "        \"shortage\", \n",
        "        \"collaboration\", \n",
        "        \"collaborate\", \n",
        "        \"human ai team\",\n",
        "        \"team\",\n",
        "        \"economic\", \n",
        "        \"economy\")\n",
        "\n",
        "queries <- paste(and, or)\n",
        "\n",
        "for (i in queries) {\n",
        "  iteration_i <- get_all_tweets(\n",
        "    query = i,\n",
        "    start_tweets = \"2022-11-30T00:00:00Z\",\n",
        "    end_tweets = \"2023-02-24T00:00:00Z\",\n",
        "    n = Inf,\n",
        "    bearer_token = bearer_token, # NOTE! the bearer token is not included in this notebook, making this code chunk impossible to run unless in possession of a Twitter API Academic account\n",
        "    data_path = \"chatgpt_full_collection/\")\n",
        "  iteration_i$query <- i  \n",
        "  chatgpt_full_collection <- bind_rows(chatgpt_full_collection, iteration_i)}\n",
        "\n",
        "saveRDS(chatgpt_full_collection, \"chatgpt_full_collection.RDS\")\n",
        "data <- readRDS(\"chatgpt_full_collection.RDS\")\n",
        "tweets <- data %>% select(text, query, created_at, author_id)\n",
        "\n",
        "file <- toJSON(tweets)\n",
        "write(file, \"full_dataset_chatgpt.json\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "RjQK48tQoYYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is little knowledge on query formulation on Twitter. Our **assumption** was **that the singular form of the word** (or its canonical form) **would return** tweets containing **the plural form too**. However, using another function from the academictwitteR package, which allows us to check for the number of tweets resulting from a query without needing to download the data, we found out **this appears not to be true**.\n",
        "\n",
        "We ran the code below to obtain the number of tweets for each of the singular/plural pairs, and saved the data. Let's load the data in the notebook so we can visualise the differences in counts between singular/plural queries.\n",
        "\n",
        "```\n",
        "and <- \"chatgpt\"\n",
        "or <- c(\"employee\",\n",
        "        \"employees\", # post collection insertion\n",
        "        \"job\", \n",
        "        \"jobs\", # post collection insertion\n",
        "        \"skill\",\n",
        "        \"skills\", # post collection insertion\n",
        "        \"concern\",\n",
        "        \"concerns\", # post collection insertion\n",
        "        \"threat\",\n",
        "        \"threats\", # post collection insertion\n",
        "        \"occupation\", # captures occupations\n",
        "        \"occupations\", # post collection insertion\n",
        "        \"earning\", # captures earnings\n",
        "        \"earnings\", # post collection insertion\n",
        "        \"task\", # captures tasks\n",
        "        \"tasks\", # post collection insertion\n",
        "        \"team\", # NEW SUGGESTION (would also capture the above)\n",
        "        \"teams\") # post collection insertion\n",
        "\n",
        "queries <- paste(and, or)\n",
        "\n",
        "query_counts <- data.frame()\n",
        "for (i in queries) {\n",
        "  iteration_i <- count_all_tweets(i,\n",
        "                                  \"2022-11-30T00:00:00Z\",\n",
        "                                  \"2023-02-24T00:00:00Z\",\n",
        "                                  bearer_token,\n",
        "                                  n = 500)\n",
        "  iteration_i$query <- i  # Create a new column \"query\" and assign it the value of i\n",
        "  query_counts <- bind_rows(query_counts, iteration_i)}\n",
        "\n",
        "saveRDS(query_counts, \"query_counts.RDS\")\n",
        "```\n",
        "\n",
        "Let's now see what we can figure out from comparing singular/plural versions of the same word."
      ],
      "metadata": {
        "id": "NxyIyun_tgrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R \n",
        "query_counts <- read_csv(\"/content/chatgpt_employment/query_counts.csv\")\n",
        "\n",
        "library(RColorBrewer)\n",
        "\n",
        "# JOB VS JOBS\n",
        "query_counts %>% \n",
        "  filter(str_detect(query, \"job\")) %>% \n",
        "ggplot(aes(x = as.Date(start), y = tweet_count, fill = query)) +\n",
        "  geom_bar(stat = \"identity\", alpha = 0.8, position = \"dodge\") +\n",
        "  scale_fill_brewer(palette = \"Set1\", name = \"Query\")+\n",
        "  theme_minimal()+\n",
        "  xlab(\"\")+\n",
        "  ylab(\"tweet count\") +\n",
        "  ggtitle(\"\")+\n",
        "  labs(subtitle = \"\")\n",
        "\n",
        "# SKILL VS SKILLS\n",
        "query_counts %>% \n",
        "  filter(str_detect(query, \"skill\")) %>% \n",
        "  ggplot(aes(x = as.Date(start), y = tweet_count, fill = query)) +\n",
        "  geom_bar(stat = \"identity\", alpha = 0.8, position = \"dodge\") +\n",
        "  scale_fill_brewer(palette = \"Set1\", name = \"Query\")+\n",
        "  theme_minimal()+\n",
        "  xlab(\"\")+\n",
        "  ylab(\"tweet count\") +\n",
        "  ggtitle(\"\")+\n",
        "  labs(subtitle = \"\")\n",
        "\n",
        "# THREAT VS THREATS\n",
        "query_counts %>% \n",
        "  filter(str_detect(query, \"threat\")) %>% \n",
        "  ggplot(aes(x = as.Date(start), y = tweet_count, fill = query)) +\n",
        "  geom_bar(stat = \"identity\", alpha = 0.8, position = \"dodge\") +\n",
        "  scale_fill_brewer(palette = \"Set1\", name = \"Query\")+\n",
        "  theme_minimal()+\n",
        "  xlab(\"\")+\n",
        "  ylab(\"tweet count\") +\n",
        "  ggtitle(\"\")+\n",
        "  labs(subtitle = \"\")\n",
        "\n",
        "# TASK VS TASKS\n",
        "query_counts %>% \n",
        "  filter(str_detect(query, \"task\")) %>% \n",
        "  ggplot(aes(x = as.Date(start), y = tweet_count, fill = query)) +\n",
        "  geom_bar(stat = \"identity\", alpha = 0.8, position = \"dodge\") +\n",
        "  scale_fill_brewer(palette = \"Set1\", name = \"Query\")+\n",
        "  theme_minimal()+\n",
        "  xlab(\"\")+\n",
        "  ylab(\"tweet count\") +\n",
        "  ggtitle(\"\")+\n",
        "  labs(subtitle = \"\")\n",
        "\n",
        "# EMPLOYEE VS EMPLOYEES\n",
        "query_counts %>% \n",
        "  filter(str_detect(query, \"employee\")) %>% \n",
        "  ggplot(aes(x = as.Date(start), y = tweet_count, fill = query)) +\n",
        "  geom_bar(stat = \"identity\", alpha = 0.8, position = \"dodge\") +\n",
        "  scale_fill_brewer(palette = \"Set1\", name = \"Query\")+\n",
        "  theme_minimal()+\n",
        "  xlab(\"\")+\n",
        "  ylab(\"tweet count\") +\n",
        "  ggtitle(\"\")+\n",
        "  labs(subtitle = \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGF1XKUt8F7i",
        "outputId": "9b7e4a1d-d470-46cc-fd8c-50e50efa5e1f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Cell magic `%%R` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the analysis above, we decided to run a second data collection for the plural form of the words.\n",
        "\n",
        "```\n",
        "library(tidyverse)\n",
        "and <- \"chatgpt\"\n",
        "or <- c(\"employees\", # post collection insertion\n",
        "        \"jobs\", # post collection insertion\n",
        "        \"skills\", # post collection insertion\n",
        "        \"concerns\", # post collection insertion\n",
        "        \"threats\", # post collection insertion\n",
        "        \"occupations\", # post collection insertion\n",
        "        \"earnings\", # post collection insertion\n",
        "        \"tasks\", # post collection insertion\n",
        "        \"collaborations\") # post collection insertion\n",
        "\n",
        "queries <- paste(and, or)\n",
        "\n",
        "chatgpt_second_collection <- data.frame()\n",
        "for (i in queries) {\n",
        "  iteration_i <- get_all_tweets(\n",
        "    query = i,\n",
        "    start_tweets = \"2022-11-30T00:00:00Z\",\n",
        "    end_tweets = \"2023-02-24T00:00:00Z\",\n",
        "    n = Inf,\n",
        "    bearer_token = bearer_token,\n",
        "    data_path = \"chatgpt_second_collection/\")\n",
        "  iteration_i$query <- i  # Create a new column \"query\" and assign it the value of i\n",
        "  chatgpt_second_collection <- bind_rows(chatgpt_second_collection, iteration_i)}\n",
        "\n",
        "saveRDS(chatgpt_second_collection, \"chatgpt_second_collection.RDS\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "c4kErsVivrfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary statistics and characteristics of the data\n",
        "\n",
        "Essentially, "
      ],
      "metadata": {
        "id": "JC_dqnpsynkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "\n",
        "# keep only unique observations\n",
        "filtered <- tweets %>%\n",
        "  group_by(text) %>%\n",
        "  slice(1)\n",
        "\n",
        "# mark retweets and identity author\n",
        "filtered <- filtered %>%\n",
        "  mutate(rt = str_detect(full_text, \"RT.*:\\\\s\"),\n",
        "         author = str_extract(full_text, \"RT.*:\\\\s\"))\n",
        "\n",
        "# how many tweets from the same person have been retweeted?\n",
        "filtered %>% ungroup() %>% select(author_id) %>% drop_na() %>% count(author_id, sort = TRUE) %>% \n",
        "  filter(n > 50) %>% \n",
        "  ggplot(aes(x = reorder(author_id, n), y = n)) +\n",
        "  geom_bar(stat = \"identity\") +\n",
        "  xlab(\"\") +\n",
        "  ylab(\"Number of retweets\")+\n",
        "  ggtitle(\"Each bar is a tweet, its height the number of times it has been retweeted\")+\n",
        "  theme(axis.text.x = element_blank())+\n",
        "  theme_minimal()\n",
        "\n",
        "# doing the same for chatgpt full dataset\n",
        "tweets <- tweets %>%\n",
        "  mutate(rt = str_detect(full_text, \"RT.*:\\\\s\"),\n",
        "         author = str_extract(full_text, \"RT.*:\\\\s\"))\n",
        "\n",
        "# visualising\n",
        "tweets %>% ungroup() %>% select(author) %>% drop_na() %>% count(author, sort = TRUE) %>% slice_head(n = 10) %>%\n",
        "  ggplot(aes(x = reorder(author, n), y = n)) +\n",
        "  geom_bar(stat = \"identity\") +\n",
        "  coord_flip() +\n",
        "  ggtitle(\"Top 10\")\n",
        "\n",
        "filtered <- filtered %>%\n",
        "  mutate(text_clean = str_remove(full_text, \"RT.*@\"))\n",
        "\n",
        "filtered %>% count(text_clean) # remove http links"
      ],
      "metadata": {
        "id": "-hcW5BbDy4OX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}